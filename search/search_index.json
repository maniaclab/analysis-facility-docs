{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started Logging in to the UChicago Analysis Facility First you will need to sign up on the Analysis Facility website . Please use your institutional or CERN identity (lxplus username) when signing up, as this will make the approval process smoother. As part of signing up you will need to upload an SSH Public Key. If you are not sure if you have generated an SSH Public Key before, try the following command (Mac/Linux) on your laptop to print the content of the file that contains the SSH Public Key: cat ~/.ssh/id_rsa.pub If the file exists, you should be able to copy the contents of this file to your profile on the AF website. Important: Do not copy the contents of a file that does not end in .pub. You must only upload the public (.pub) part of the key. If you do not have a public key (the file doesn't exist), you can generate one via the following command (Mac/Linux): ssh-keygen -t rsa Upload the resulting public key (ending in .pub) to your profile. Once you have uploaded a key, it will take a little bit of time to process your profile and add your account to the system. After 10-15 minutes, you ought to be able to login via SSH: ssh <username>@login.af.uchicago.edu If it does not work, please double check that you have been approved, have a public key uploaded and have waited at least 15 minutes. If you still have an issue, feel free to reach out to us for help. Using Analysis Facility Filesystems The UChicago analysis facility has three filesystems that you should be aware of when running workloads. The table below describes their differences: Filesystem Quota Path Backup Notes $HOME 100 GB /home/$USER Yes Solid-state filesystem, shared to all worker nodes $DATA 10 TB /data/$USER No CephFS filesystem, shared to all worker nodes $SCRATCH n/a /scratch No Ephemeral storage for workloads, local to worker nodes","title":"Home"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#logging-in-to-the-uchicago-analysis-facility","text":"First you will need to sign up on the Analysis Facility website . Please use your institutional or CERN identity (lxplus username) when signing up, as this will make the approval process smoother. As part of signing up you will need to upload an SSH Public Key. If you are not sure if you have generated an SSH Public Key before, try the following command (Mac/Linux) on your laptop to print the content of the file that contains the SSH Public Key: cat ~/.ssh/id_rsa.pub If the file exists, you should be able to copy the contents of this file to your profile on the AF website. Important: Do not copy the contents of a file that does not end in .pub. You must only upload the public (.pub) part of the key. If you do not have a public key (the file doesn't exist), you can generate one via the following command (Mac/Linux): ssh-keygen -t rsa Upload the resulting public key (ending in .pub) to your profile. Once you have uploaded a key, it will take a little bit of time to process your profile and add your account to the system. After 10-15 minutes, you ought to be able to login via SSH: ssh <username>@login.af.uchicago.edu If it does not work, please double check that you have been approved, have a public key uploaded and have waited at least 15 minutes. If you still have an issue, feel free to reach out to us for help.","title":"Logging in to the UChicago Analysis Facility"},{"location":"#using-analysis-facility-filesystems","text":"The UChicago analysis facility has three filesystems that you should be aware of when running workloads. The table below describes their differences: Filesystem Quota Path Backup Notes $HOME 100 GB /home/$USER Yes Solid-state filesystem, shared to all worker nodes $DATA 10 TB /data/$USER No CephFS filesystem, shared to all worker nodes $SCRATCH n/a /scratch No Ephemeral storage for workloads, local to worker nodes","title":"Using Analysis Facility Filesystems"},{"location":"coffeacasa/","text":"Coffea Casa Prerequisites The primary mode of analysis with coffea-casa is coffea . Coffea provides plenty of examples to users in its documentation A good starting point may be this tutorial on columnar analysis in coffea. Knowledge of Python is also assumed. Standard coffea analyses are contained within Jupyter Notebooks , which allow for dynamic, block-by-block execution of code. Coffea-casa employs the JupyterLab interface. JupyterLab is designed for hosting Jupyter Notebooks on the web, and permits the usage of additional features within its environment, including Git access, compatibility with cluster computing tools, and much, much more. If you aren't familiar with any of these tools, please click on the links above for additional resources, and get acquainted with how they work. If you want examples of how coffea-casa merges these tools, refer to the gallery of coffea-casa examples. Access Please use https://coffea.af.uchicago.edu as an access point to the Coffea-Casa Analysis Facility @ UChicago. ATLAS AuthZ Authentication Instance Currently Coffea-Casa Analysis Facility @ UChicago can support any member of ATLAS. Sign in with your ATLAS CERN credential: Docker Image Selection For high efficient analysis using coffea package, powered with Dask and HTCondor please select: After you will be forwarded to your personal Jupyterhub instance running at Analysis Facility @ UChicago: Cluster Resources in Coffea-Casa Analysis Facility @ UChicago By default, the Coffea-casa Dask cluster should provide you with a scheduler and workers, which you can see by clicking on the colored Dask icon in the left sidebar. As soon as you will start your computations, you will notice that available resources at the Coffea-Casa Analysis Facility @ UChicago can easily autoscale depending on available resources in the HTCondor pool at AF UChicago. Opening a New Console or File There are three ways by which you can open a new tab within coffea-casa. Two are located within the File menu at the very top of the JupyterLab interface: New and New Launcher. The New dropdown menu allows you to open the console or a file of a specified format directly. The New Launcher option creates a new tab with buttons that permit you to launch a console or a new file, exactly like the interface you are shown when you first open coffea-casa. The final way is specific to the File Browser tab of the sidebar. This behaves exactly like the New Launcher option above. Regardless of the method you use to open a new file, the file will be saved to the current directory of your **File Browser.** Using Git Cloning a repository in the Coffea-casa Analysis Facility @ UChicago is simple, though it can be a little confusing because it is spread across two tabs in the sidebar: the File Browser and the Git tabs. In order to clone a repository, first go to the Git tab. It should look like this: Simply click the appropriate button (initialize a repository, or clone a repository) and you'll be hooked up to GitHub. This should then take you to the File Browser tab, which is where you can see all of the repositories you have cloned in your JupyterLab instance. The File Browser should look like this: If you wish to change repositories, simply click the folder button to enter the root directory. If you are in the root directory, the Git tab will reset and allow you to clone another repository. If you wish to commit, push, or pull from the repository you currently have active in the File Browser, then you can return to the Git tab. It should change to look like this, so long as you have a repository open in the File Browser: The buttons in the top right allow for pulling and pushing respectively. When you have edited files in a directory, they will show up under the Changed category, at which point you can hit the + to add them to a commit (at which point they will show up under Staged ). Filling out the box at the bottom of the sidebar will file your commit, and prepare it for you to push.","title":"Basics"},{"location":"coffeacasa/#coffea-casa","text":"","title":"Coffea Casa"},{"location":"coffeacasa/#prerequisites","text":"The primary mode of analysis with coffea-casa is coffea . Coffea provides plenty of examples to users in its documentation A good starting point may be this tutorial on columnar analysis in coffea. Knowledge of Python is also assumed. Standard coffea analyses are contained within Jupyter Notebooks , which allow for dynamic, block-by-block execution of code. Coffea-casa employs the JupyterLab interface. JupyterLab is designed for hosting Jupyter Notebooks on the web, and permits the usage of additional features within its environment, including Git access, compatibility with cluster computing tools, and much, much more. If you aren't familiar with any of these tools, please click on the links above for additional resources, and get acquainted with how they work. If you want examples of how coffea-casa merges these tools, refer to the gallery of coffea-casa examples.","title":"Prerequisites"},{"location":"coffeacasa/#access","text":"Please use https://coffea.af.uchicago.edu as an access point to the Coffea-Casa Analysis Facility @ UChicago.","title":"Access"},{"location":"coffeacasa/#atlas-authz-authentication-instance","text":"Currently Coffea-Casa Analysis Facility @ UChicago can support any member of ATLAS. Sign in with your ATLAS CERN credential:","title":"ATLAS AuthZ Authentication Instance"},{"location":"coffeacasa/#docker-image-selection","text":"For high efficient analysis using coffea package, powered with Dask and HTCondor please select: After you will be forwarded to your personal Jupyterhub instance running at Analysis Facility @ UChicago:","title":"Docker Image Selection"},{"location":"coffeacasa/#cluster-resources-in-coffea-casa-analysis-facility-uchicago","text":"By default, the Coffea-casa Dask cluster should provide you with a scheduler and workers, which you can see by clicking on the colored Dask icon in the left sidebar. As soon as you will start your computations, you will notice that available resources at the Coffea-Casa Analysis Facility @ UChicago can easily autoscale depending on available resources in the HTCondor pool at AF UChicago.","title":"Cluster Resources in Coffea-Casa Analysis Facility @ UChicago"},{"location":"coffeacasa/#opening-a-new-console-or-file","text":"There are three ways by which you can open a new tab within coffea-casa. Two are located within the File menu at the very top of the JupyterLab interface: New and New Launcher. The New dropdown menu allows you to open the console or a file of a specified format directly. The New Launcher option creates a new tab with buttons that permit you to launch a console or a new file, exactly like the interface you are shown when you first open coffea-casa. The final way is specific to the File Browser tab of the sidebar. This behaves exactly like the New Launcher option above. Regardless of the method you use to open a new file, the file will be saved to the current directory of your **File Browser.**","title":"Opening a New Console or File"},{"location":"coffeacasa/#using-git","text":"Cloning a repository in the Coffea-casa Analysis Facility @ UChicago is simple, though it can be a little confusing because it is spread across two tabs in the sidebar: the File Browser and the Git tabs. In order to clone a repository, first go to the Git tab. It should look like this: Simply click the appropriate button (initialize a repository, or clone a repository) and you'll be hooked up to GitHub. This should then take you to the File Browser tab, which is where you can see all of the repositories you have cloned in your JupyterLab instance. The File Browser should look like this: If you wish to change repositories, simply click the folder button to enter the root directory. If you are in the root directory, the Git tab will reset and allow you to clone another repository. If you wish to commit, push, or pull from the repository you currently have active in the File Browser, then you can return to the Git tab. It should change to look like this, so long as you have a repository open in the File Browser: The buttons in the top right allow for pulling and pushing respectively. When you have edited files in a directory, they will show up under the Changed category, at which point you can hit the + to add them to a commit (at which point they will show up under Staged ). Filling out the box at the bottom of the sidebar will file your commit, and prepare it for you to push.","title":"Using Git"},{"location":"job_submission/","text":"Submitting to the Analysis Facility The UChicago Analysis Facility uses HTCondor for batch workloads. You will need to define an executable script and a \"submit\" file that describes your job. A simple job that loads the ATLAS environment looks something like this: Job script, called myjob.sh: #!/bin/bash export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase export ALRB_localConfigDir=$HOME/localConfig source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh # at this point, you can lsetup root, rucio, athena, etc.. Submit file, called myjob.sub: Universe = vanilla Output = myjob.$(Cluster).$(Process).out Error = myjob.$(Cluster).$(Process).err Log = myjob.log Executable = myjob.sh request_memory = 1GB request_cpus = 1 Queue 1 The condor_submit command is used to queue jobs: $ condor_submit myjob.sub Submitting job(s). 1 job(s) submitted to cluster 17. And the condor_q command is used to view the queue: [lincolnb@login01 ~]$ condor_q -- Schedd: head01.af.uchicago.edu : &lt;192.170.240.14:9618?... @ 07/22/21 11:28:26 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS lincolnb ID: 17 7/22 11:27 _ 1 _ 1 17.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for lincolnb: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Configuring your jobs to use an X509 Proxy Certificate If you need to use an X509 Proxy, e.g. to access ATLAS Data, you will want to copy your X509 certificate to the Analysis Facility. Store your certificate in $HOME/.globus and create a ATLAS VOMS proxy in the usual way: export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase export ALRB_localConfigDir=$HOME/localConfig source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh lsetup emi voms-proxy-init -voms atlas -out $HOME/x509proxy You will want to generate the proxy on, or copy it to, the shared $HOME filesystem so that the HTCondor scheduler can find and read the proxy. With the following additions to your jobscript, HTCondor will configure the job enviroment automatically for X509 authenticated data access: use_x509userproxy = true x509userproxy = /home/YOURUSERNAME/x509proxy E.g., in the job above for the user lincolnb: Universe = vanilla Output = myjob.$(Cluster).$(Process).out Error = myjob.$(Cluster).$(Process).err Log = myjob.log Executable = myjob.sh use_x509userproxy = true x509userproxy = /home/lincolnb/x509proxy request_memory = 1GB request_cpus = 1 Queue 1 Using Analysis Facility Filesystems When submitting jobs, you should try to use the local scratch filesystem whenever possible. This will help you be a \"good neighbor\" to other users on the system, and reduce overall stress on the shared filesystems, which can lead to slowness, downtimes, etc. By default, jobs start in the $SCRATCH directory on the worker nodes. Output data will need to be staged to the shared filesystem or it will be lost! In the following example, data is read from Rucio, we pretend to process it, and then push a small output copied back to the $HOME filesystem. It assumes your X509 proxy certificate is valid and in your home directory. #!/bin/bash export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase export ALRB_localConfigDir=$HOME/localConfig source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh lsetup rucio rucio --verbose download --rse MWT2_DATADISK data16_13TeV:AOD.11071822._001488.pool.root.1 # You can run things like asetup as well asetup AnalysisBase,21.2.81 # This is where you would do your data analysis via AnalysisBase, etc. We will # just pretend to do that, and truncate the file to simulate generating an # output file. This is definitely not what you want to do in a real analysis! cd data16_13TeV truncate --size 10MB AOD.11071822._001488.pool.root.1 cp AOD.11071822._001488.pool.root.1 $HOME/myjob.output It gets submitted in the usual way: Universe = vanilla Output = myjob.$(Cluster).$(Process).out Error = myjob.$(Cluster).$(Process).err Log = myjob.log Executable = myjob.sh use_x509userproxy = true x509userproxy = /home/lincolnb/x509proxy request_memory = 1GB request_cpus = 1 Queue 1 And then: $ condor_submit myjob.sub Submitting job(s). 1 job(s) submitted to cluster 17. Using Docker / Singularity containers (Advanced) Some users may want to bring their own container-based workloads to the Analysis Facility. We support both Docker-based jobs as well as Singularity-based jobs. Additionally, the CVMFS repository unpacked.cern.ch is mounted on all nodes. If, for whatever reason, you wanted to run a Debian Linux-based container on the Analysis Facilty, it would be as simple as the following Job file: universe = docker docker_image = debian executable = /bin/cat arguments = /etc/hosts should_transfer_files = YES when_to_transfer_output = ON_EXIT output = out.$(Process) error = err.$(Process) log = log.$(Process) request_memory = 1000M queue 1 Similarly, if you would like to run a Singularity container, such as the ones provided in th unpacked.cern.ch CVMFS repo, you can submit a normal vanilla universe job, with a job executable that looks something like the following: #!/bin/bash singularity run -B /cvmfs -B /home /cvmfs/unpacked.cern.ch/registry.hub.docker.com/atlas/rucio-clients:default rucio --version Replacing the rucio-clients:default container and rucio --version executable with your preferred software.","title":"Job submission"},{"location":"job_submission/#submitting-to-the-analysis-facility","text":"The UChicago Analysis Facility uses HTCondor for batch workloads. You will need to define an executable script and a \"submit\" file that describes your job. A simple job that loads the ATLAS environment looks something like this: Job script, called myjob.sh: #!/bin/bash export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase export ALRB_localConfigDir=$HOME/localConfig source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh # at this point, you can lsetup root, rucio, athena, etc.. Submit file, called myjob.sub: Universe = vanilla Output = myjob.$(Cluster).$(Process).out Error = myjob.$(Cluster).$(Process).err Log = myjob.log Executable = myjob.sh request_memory = 1GB request_cpus = 1 Queue 1 The condor_submit command is used to queue jobs: $ condor_submit myjob.sub Submitting job(s). 1 job(s) submitted to cluster 17. And the condor_q command is used to view the queue: [lincolnb@login01 ~]$ condor_q -- Schedd: head01.af.uchicago.edu : &lt;192.170.240.14:9618?... @ 07/22/21 11:28:26 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS lincolnb ID: 17 7/22 11:27 _ 1 _ 1 17.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for lincolnb: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended","title":"Submitting to the Analysis Facility"},{"location":"job_submission/#configuring-your-jobs-to-use-an-x509-proxy-certificate","text":"If you need to use an X509 Proxy, e.g. to access ATLAS Data, you will want to copy your X509 certificate to the Analysis Facility. Store your certificate in $HOME/.globus and create a ATLAS VOMS proxy in the usual way: export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase export ALRB_localConfigDir=$HOME/localConfig source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh lsetup emi voms-proxy-init -voms atlas -out $HOME/x509proxy You will want to generate the proxy on, or copy it to, the shared $HOME filesystem so that the HTCondor scheduler can find and read the proxy. With the following additions to your jobscript, HTCondor will configure the job enviroment automatically for X509 authenticated data access: use_x509userproxy = true x509userproxy = /home/YOURUSERNAME/x509proxy E.g., in the job above for the user lincolnb: Universe = vanilla Output = myjob.$(Cluster).$(Process).out Error = myjob.$(Cluster).$(Process).err Log = myjob.log Executable = myjob.sh use_x509userproxy = true x509userproxy = /home/lincolnb/x509proxy request_memory = 1GB request_cpus = 1 Queue 1","title":"Configuring your jobs to use an X509 Proxy Certificate"},{"location":"job_submission/#using-analysis-facility-filesystems","text":"When submitting jobs, you should try to use the local scratch filesystem whenever possible. This will help you be a \"good neighbor\" to other users on the system, and reduce overall stress on the shared filesystems, which can lead to slowness, downtimes, etc. By default, jobs start in the $SCRATCH directory on the worker nodes. Output data will need to be staged to the shared filesystem or it will be lost! In the following example, data is read from Rucio, we pretend to process it, and then push a small output copied back to the $HOME filesystem. It assumes your X509 proxy certificate is valid and in your home directory. #!/bin/bash export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase export ALRB_localConfigDir=$HOME/localConfig source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh lsetup rucio rucio --verbose download --rse MWT2_DATADISK data16_13TeV:AOD.11071822._001488.pool.root.1 # You can run things like asetup as well asetup AnalysisBase,21.2.81 # This is where you would do your data analysis via AnalysisBase, etc. We will # just pretend to do that, and truncate the file to simulate generating an # output file. This is definitely not what you want to do in a real analysis! cd data16_13TeV truncate --size 10MB AOD.11071822._001488.pool.root.1 cp AOD.11071822._001488.pool.root.1 $HOME/myjob.output It gets submitted in the usual way: Universe = vanilla Output = myjob.$(Cluster).$(Process).out Error = myjob.$(Cluster).$(Process).err Log = myjob.log Executable = myjob.sh use_x509userproxy = true x509userproxy = /home/lincolnb/x509proxy request_memory = 1GB request_cpus = 1 Queue 1 And then: $ condor_submit myjob.sub Submitting job(s). 1 job(s) submitted to cluster 17.","title":"Using Analysis Facility Filesystems"},{"location":"job_submission/#using-docker-singularity-containers-advanced","text":"Some users may want to bring their own container-based workloads to the Analysis Facility. We support both Docker-based jobs as well as Singularity-based jobs. Additionally, the CVMFS repository unpacked.cern.ch is mounted on all nodes. If, for whatever reason, you wanted to run a Debian Linux-based container on the Analysis Facilty, it would be as simple as the following Job file: universe = docker docker_image = debian executable = /bin/cat arguments = /etc/hosts should_transfer_files = YES when_to_transfer_output = ON_EXIT output = out.$(Process) error = err.$(Process) log = log.$(Process) request_memory = 1000M queue 1 Similarly, if you would like to run a Singularity container, such as the ones provided in th unpacked.cern.ch CVMFS repo, you can submit a normal vanilla universe job, with a job executable that looks something like the following: #!/bin/bash singularity run -B /cvmfs -B /home /cvmfs/unpacked.cern.ch/registry.hub.docker.com/atlas/rucio-clients:default rucio --version Replacing the rucio-clients:default container and rucio --version executable with your preferred software.","title":"Using Docker / Singularity containers (Advanced)"},{"location":"ml_platform/","text":"ML platform The atlas-ml.org service, part of the ATLAS Distributed Computing analytics and monitoring effort, was developed to provide GPU resources for algorithm development for system metadata collected by the Elasticsearch cluster at UChicago.","title":"ML platform"},{"location":"ml_platform/#ml-platform","text":"The atlas-ml.org service, part of the ATLAS Distributed Computing analytics and monitoring effort, was developed to provide GPU resources for algorithm development for system metadata collected by the Elasticsearch cluster at UChicago.","title":"ML platform"},{"location":"servicex/","text":"ServiceX When dealing with very large datasets it is often better to do initial data filtering, augmentation using ServiceX <https://iris-hep.org/projects/servicex> _. ServiceX transformation produces output as an Awkward Array. The array can then be used in a regular Coffea processing. Here a scheme explaining the workflow: There are two different, UC AF deployed ServiceX instances. The only difference between them is the type of input data they are capable of processing. Uproot processes any kind of \"flat\" ROOT files, while xAOD processes only Rucio registered xAOD files. To use them one has to register and get approved. Sign in will lead you to a Globus registration page, where you may choose to use account connected to your institution: Once approved, you will be able to see status of your requests in the dashboard: For your code to be able to authenticate your requests, you need to download a servicex.yaml file, that should be placed in your working directory. The file is downloaded from your profile page: For an example analysis using ServiceX and Coffea look here. More Examples Columnar data analysis with DAOD_PHYSLITE here. ServiceX analysis on ROOT files, with Coffea, and TRExFitter here.","title":"ServiceX"},{"location":"servicex/#servicex","text":"When dealing with very large datasets it is often better to do initial data filtering, augmentation using ServiceX <https://iris-hep.org/projects/servicex> _. ServiceX transformation produces output as an Awkward Array. The array can then be used in a regular Coffea processing. Here a scheme explaining the workflow: There are two different, UC AF deployed ServiceX instances. The only difference between them is the type of input data they are capable of processing. Uproot processes any kind of \"flat\" ROOT files, while xAOD processes only Rucio registered xAOD files. To use them one has to register and get approved. Sign in will lead you to a Globus registration page, where you may choose to use account connected to your institution: Once approved, you will be able to see status of your requests in the dashboard: For your code to be able to authenticate your requests, you need to download a servicex.yaml file, that should be placed in your working directory. The file is downloaded from your profile page: For an example analysis using ServiceX and Coffea look here.","title":"ServiceX"},{"location":"servicex/#more-examples","text":"Columnar data analysis with DAOD_PHYSLITE here. ServiceX analysis on ROOT files, with Coffea, and TRExFitter here.","title":"More Examples"},{"location":"xcache/","text":"XCache Analysis Facility maintains an XCache server (managed through SLATE), with 25 x 1.5 TB NVMes and 2x25 Gbps NIC. ServiceX uses the XCache by default. Users can manually add the prefix root://192.170.240.18:1094// to their root paths, eg: If the original path is: root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/user/mgeyik/63/c4/user.mgeyik.26617246._000006.out.root make it: root://192.170.240.18:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/user/mgeyik/63/c4/user.mgeyik.26617246._000006.out.root Alternatively you may use script xcache_ls to get the cfile containing the best paths to access all the files in their dataset.","title":"XCache"},{"location":"xcache/#xcache","text":"Analysis Facility maintains an XCache server (managed through SLATE), with 25 x 1.5 TB NVMes and 2x25 Gbps NIC. ServiceX uses the XCache by default. Users can manually add the prefix root://192.170.240.18:1094// to their root paths, eg: If the original path is: root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/user/mgeyik/63/c4/user.mgeyik.26617246._000006.out.root make it: root://192.170.240.18:1094//root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/user/mgeyik/63/c4/user.mgeyik.26617246._000006.out.root Alternatively you may use script xcache_ls to get the cfile containing the best paths to access all the files in their dataset.","title":"XCache"}]}